\section{Data Colllection} \label{sec:DataCollection}

As of now, we have only collected data from the Web Advanced Development which contains the following variables: Name District. We have identified three main methods to collect more data about the city of Jönköping.

\textbf{Option 1: The Internet }


Our first option is to utilize the web browser to collect more data. The data that we find on the internet can be described as both qualitative (Jönköping's districts, educational institutions, or community services) and quantitative (numerical data) resources.


\textbf{Option 2: Surveys}


The second option is to create surveys for both students and teachers. The aim is to gain information about the city which cannot be found on the internet. The answers from the survey can be classed as both quantitative and qualitative. 

\textbf{Option 3: Interviews}


Our third option is conducting interviews, which is a qualitative method. We will interview current and former employees of different stores and buildings in the city of Jönköping.
 
\section{Data Collection Process}

Originally, we received a dataset from the Advanced Web Development course, comprising 138 distinct stores situated in Jönköping. Each store is accompanied by its name and district.

\vspace{6pt} 

Three variables are insufficient for the project's requirements, necessitating an expansion of our dataset. Two approaches were employed to acquire additional variables for the 138 stores. The first method involved online research, utilizing \cite{NotavailableAllaBolag}, which furnishes data about specific companies/stores. The new variables gathered include Revenue, Yearly Result, Result after financial net, Total assets, Profit margin, Solvency, Cash flow, and Gross profit margin. Each value was extracted from a financial report generated in December 2022. The second online resource utilized was \cite{NotavailableGoogleMaps}, aiming to incorporate two additional variables: longitude and latitude for each store. 
\vspace{6pt} 

We aimed to introduce two more variables, namely "Number of employees" and "Number of department stores in the country". However, our online research yielded no data for these variables. Therefore, we have devised an alternative approach: conducting interviews. Our plan involves visiting each store and interviewing current or former employees to gather information on the number of employees and the number of department stores in the country.

\vspace{6pt}
The dataset is projected to undergo substantial growth as we integrate data from forthcoming interviews. Originally encompassing solely two variables - Store name and District - it is expected to expand to encompass a comprehensive set of 14 variables. These will entail Store name, District, Revenue, Yearly Result, Result after financial net, Total assets, Profit margin, Solvency, Cash flow, Gross profit margin, Number of employees, Number of department stores in the country, Longitude, and Latitude coordinates, in addition to the newly acquired information acquired through interviews.

\section{Data Analysis}
Currently, we have two options when it comes to implementing and analyzing data we will collect, we could either use Python or normal HTML, CSS, and JavaScript. The authors are both experienced in both options. 

\textbf{Option 1: Python}


If we were to use Python, the data collected would be stored in MariaDB which is a branch of MySQL. When it comes to database operations, we will use SQLAlchemy or Djangos Object-relational Mapper (ORM). The backend will use Flask or Django to create API endpoints to interact with MariaDB.
Python libraries such as Scikit-learn or TensorFlow will be used to develop prediction algorithms. The backend can process data fetched from MariaDB, apply the prediction model, and send results to the frontend.
When it comes to the feedback mechanism, API endpoints will be implemented to receive user feedback on predictions, which can be used to refine the model.

\textbf{Option 2: HTML, CSS and JavaScript}


The second option will use HTML, CSS, and JavaScript. HTML will be used to design the user interface and then style it with CSS, this includes forms for the user inputs and sections to display predictions.  JavaScript offers different libraries or frameworks such as React or Vue.js, one of them will be used as a framework. It will be used to make the web application interactive, but also to fetch data from the backend using AJAX or Fetch API. Lastly, display predictions, and handle user feedback submissions.


Both options should aim to produce a dynamic application where predictions can be made based on the data stored in the database, and where it will be presented through a web interface and refined through ongoing feedback.

\section{Validity and Reliability}
From the start, the validity and reliability aspects of our research have been very important, these factors have guided us in choosing how we collect data, analyze it, and pick our theories. Below, we will look through and explain the choices we made which will show that our results are trustworthy and solid.  


\textbf{Validity:}


The validity aspect of the research is to make sure that our methods for collecting and analyzing data are crucial for the accuracy of our study’s conclusions.  For example, when it comes to the surveys, we need to craft them to ensure that each question accurately measures the intended variables. This careful consideration reduces the risk of collecting data that do not reflect what we intended, even if the survey is carried out perfectly. Similarly, choosing the DECAS theory and Tamara Munzner's What-Why-How framework was based on how well they fit and could be applied to our data, ensuring that our research is built on theories that match our goals.


\textbf{Reliability:}


The reliability aspect of the research depends on how consistent our data gathering tools are and whether others can achieve the same result by repeating our study. For us to make sure that other researchers find similar conclusions when they conduct the same research on our study, we have used standard methods such as well-organized surveys and specific interviews. The analyzing part uses well-known computing methods and clear algorithms which also makes our findings more reliable.
\section{Considerations}

During the research, we have carefully considered various important factors to ensure the study’s relevance, integrity, and impact. The considerations made at these factors were crucial, to make sure that the research methodology and findings aligned well with wider scientific, and ethical standards. 
\textbf{Scientific Considerations}

With our approach, we aim to apply the DECAS theory and Tamara Munzner’s What-Why-How framework to our collected dataset by using thorough methods for analyzing and interpreting data. To create a solid, clear, reproducible approach, with the result and findings we try to contribute to the fields of computer science and data visualization.


\textbf{Ethical Considerations}

The ethical aspect of the study plays a big part in collecting data for our dataset, we will try to obtain information about the city of Jönköping through Surveys and Interviews. We will only collect and add if the participant has given their consent to provide the information, if they do not consent then the data will not be added to our dataset. Lastly, our third method to obtain data is through the Internet. When selecting information from the web we will evaluate the information provided by the source to avoid dissemination of misinformation. 


